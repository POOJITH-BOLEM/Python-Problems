{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python script that:\n",
    "1. Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lokes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lokes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lokes\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human languages, making it possible for computers to perform tasks like language translation, sentiment analysis, and more.\n",
      "\n",
      "\n",
      "Tokenized Text:\n",
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'ai', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'it', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'languages', ',', 'making', 'it', 'possible', 'for', 'computers', 'to', 'perform', 'tasks', 'like', 'language', 'translation', ',', 'sentiment', 'analysis', ',', 'and', 'more', '.']\n",
      "\n",
      "Tokens without Stopwords:\n",
      "['is', 'a', 'of', 'that', 'on', 'the', 'between', 'and', 'it', 'to', 'and', 'it', 'for', 'to', 'and', 'more']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['is', 'a', 'of', 'that', 'on', 'the', 'between', 'and', 'it', 'to', 'and', 'it', 'for', 'to', 'and', 'more']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['is', 'a', 'of', 'that', 'on', 'the', 'between', 'and', 'it', 'to', 'and', 'it', 'for', 'to', 'and', 'more']\n",
      "\n",
      "Processed Text with Gensim Custom Filters:\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'ai', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', 'it', 'enables', 'machines', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'languages', 'making', 'it', 'possible', 'for', 'computers', 'to', 'perform', 'tasks', 'like', 'language', 'translation', 'sentiment', 'analysis', 'and', 'more']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, strip_punctuation, strip_numeric, stem_text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load and read a sample text file\n",
    "file_path = \"sample_text.txt\"  # Replace with the path to your file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "\n",
    "# Step 2: Remove stopwords\n",
    "tokens_no_stopwords = [word for word in tokens if word not in remove_stopwords(word)]\n",
    "\n",
    "# Step 3: Stemming\n",
    "stemmed_tokens = [stem_text(word) for word in tokens_no_stopwords]\n",
    "\n",
    "# Step 4: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "# Optional: Combine custom Gensim filters\n",
    "custom_filters = [strip_punctuation, strip_numeric, lambda x: x.lower()]\n",
    "processed_text = preprocess_string(text, custom_filters)\n",
    "\n",
    "# Results\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nTokenized Text:\")\n",
    "print(tokens)\n",
    "print(\"\\nTokens without Stopwords:\")\n",
    "print(tokens_no_stopwords)\n",
    "print(\"\\nStemmed Tokens:\")\n",
    "print(stemmed_tokens)\n",
    "print(\"\\nLemmatized Tokens:\")\n",
    "print(lemmatized_tokens)\n",
    "print(\"\\nProcessed Text with Gensim Custom Filters:\")\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
